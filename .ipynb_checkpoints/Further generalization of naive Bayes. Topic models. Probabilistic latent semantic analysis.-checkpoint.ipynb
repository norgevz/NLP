{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import collections\n",
    "import glob\n",
    "import codecs\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "translator = str.maketrans(\"\",\"\", string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = nltk.stem.SnowballStemmer('english')\n",
    "def my_tokenizer(s):\n",
    "    return [ stemmer.stem(x) for x in nltk.tokenize.word_tokenize(s.lower().translate(translator)) if not x in stopwords ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(input='filename', encoding='latin1', min_df=20, tokenizer=my_tokenizer)\n",
    "X = cv.fit_transform([x for directory in glob.glob('20news-bydate-train/*') for x in glob.glob(directory + '/*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(input='filename', encoding='latin1', min_df=10, max_df=200, ngram_range=(1, 2), tokenizer=my_tokenizer)\n",
    "X = tfv.fit_transform([x for directory in glob.glob('20news-bydate-train/*') for x in glob.glob(directory + '/*')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<11314x22004 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 623753 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "dictionary = Dictionary.from_corpus(corpus, id2word=dict((id, word) for word, id in tfv.vocabulary_.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2321"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id['atheism']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lsi_model = gensim.models.LsiModel(corpus, id2word=dictionary, num_topics=5)\n",
    "lda_model = gensim.models.LdaModel(corpus, id2word=dictionary, num_topics=5, alpha=.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.001*\"netcom\" + 0.001*\"organ netcom\" + 0.001*\"guest\" + 0.001*\"netcom onlin\" + 0.001*\"gordon bank\" + 0.001*\"communic servic\" + 0.001*\"abc\" + 0.001*\"keenan\" + 0.001*\"gari\" + 0.001*\"gordon\"'),\n",
       " (1,\n",
       "  '0.003*\"gordon bank\" + 0.002*\"gordon\" + 0.002*\"gebcspittedu\" + 0.002*\"gebcspittedu gordon\" + 0.001*\"nhl\" + 0.001*\"playoff\" + 0.001*\"ticket\" + 0.001*\"captain\" + 0.001*\"msg\" + 0.001*\"diet\"'),\n",
       " (2,\n",
       "  '0.002*\"msg\" + 0.002*\"diseas\" + 0.001*\"penguin\" + 0.001*\"cancer\" + 0.001*\"leaf\" + 0.001*\"patient\" + 0.001*\"devil\" + 0.001*\"gordon\" + 0.001*\"gerald\" + 0.001*\"ice\"'),\n",
       " (3,\n",
       "  '0.002*\"playoff\" + 0.001*\"stat\" + 0.001*\"gari l\" + 0.001*\"0 1\" + 0.001*\"leaf\" + 0.001*\"dare\" + 0.001*\"1 1\" + 0.001*\"espn\" + 0.001*\"wing\" + 0.001*\"keller\"'),\n",
       " (4,\n",
       "  '0.002*\"armenian\" + 0.002*\"turkish\" + 0.001*\"nhl\" + 0.001*\"arab\" + 0.001*\"greek\" + 0.001*\"armenia\" + 0.001*\"palestinian\" + 0.001*\"firearm\" + 0.001*\"soldier\" + 0.001*\"orbit\"')]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.199*\"armenian\" + -0.152*\"gordon\" + -0.150*\"gordon bank\" + -0.102*\"gebcspittedu gordon\" + -0.102*\"gebcspittedu\" + -0.100*\"turkish\" + -0.067*\"scsi\" + -0.064*\"armenia\" + -0.061*\"muslim\" + -0.061*\"turk\"'),\n",
       " (1,\n",
       "  '0.395*\"gordon bank\" + 0.378*\"gordon\" + 0.269*\"gebcspittedu gordon\" + 0.269*\"gebcspittedu\" + -0.147*\"armenian\" + 0.135*\"n3jxp skeptic\" + 0.135*\"chastiti intellect\" + 0.135*\"n3jxp\" + 0.135*\"bank n3jxp\" + 0.135*\"skeptic chastiti\"'),\n",
       " (2,\n",
       "  '0.520*\"armenian\" + 0.252*\"turkish\" + 0.171*\"armenia\" + 0.170*\"argic\" + 0.168*\"serdar\" + 0.166*\"serdar argic\" + 0.157*\"turk\" + 0.112*\"turkey\" + 0.111*\"genocid\" + 0.103*\"serazumauucp\"'),\n",
       " (3,\n",
       "  '-0.462*\"scsi\" + -0.268*\"ide\" + -0.127*\"hard drive\" + -0.113*\"simm\" + -0.104*\"isa\" + 0.101*\"islam\" + -0.098*\"motherboard\" + -0.096*\"printer\" + -0.095*\"armenian\" + -0.091*\"scsi2\"'),\n",
       " (4,\n",
       "  '0.242*\"islam\" + 0.239*\"scsi\" + 0.168*\"sandviknewtonapplecom\" + 0.164*\"kent\" + 0.141*\"ide\" + 0.112*\"muslim\" + 0.104*\"schneider\" + 0.099*\"sandvik\" + 0.099*\"kent sandvik\" + 0.097*\"sandviknewtonapplecom kent\"')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 0.42333399318482828), (4, 0.56645592009405998)]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[doc]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
