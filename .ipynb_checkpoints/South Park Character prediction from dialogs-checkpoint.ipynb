{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import numpy as np\n",
    "TRAIN_FILE = 'data/test1/SouthPark.train.tsv'\n",
    "TEST_FILE = 'data/test1/SouthPark.test.tsv'\n",
    "epochs = 20\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>episode</th>\n",
       "      <th>character</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Stan</td>\n",
       "      <td>Throw it here! Throw it here!  Yeah! Nice one,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Dude, that sucks, Clyde. A mom shouldn't be ab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Kyle</td>\n",
       "      <td>Is your mom always like that dude?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Of course, man. It's cool.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Cartman</td>\n",
       "      <td>Clyde, Clyde! What have I told you about pissi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   season  episode character  \\\n",
       "0      16        1      Stan   \n",
       "1      16        1   Cartman   \n",
       "2      16        1      Kyle   \n",
       "3      16        1   Cartman   \n",
       "4      16        1   Cartman   \n",
       "\n",
       "                                                text  \n",
       "0  Throw it here! Throw it here!  Yeah! Nice one,...  \n",
       "1  Dude, that sucks, Clyde. A mom shouldn't be ab...  \n",
       "2                 Is your mom always like that dude?  \n",
       "3                         Of course, man. It's cool.  \n",
       "4  Clyde, Clyde! What have I told you about pissi...  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(TRAIN_FILE,  sep='\\t', header=None)\n",
    "train_df.columns = ['season', 'episode', 'character', 'text']\n",
    "\n",
    "test_df = pd.read_csv(TEST_FILE,  sep='\\t', header=None)\n",
    "test_df.columns = ['season', 'episode', 'character', 'text']\n",
    "\n",
    "test_df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Announcer': 7,\n",
       " 'Butters': 18,\n",
       " 'Cartman': 3,\n",
       " 'Chef': 19,\n",
       " 'Gerald': 0,\n",
       " 'Jimbo': 5,\n",
       " 'Jimmy': 12,\n",
       " 'Kenny': 11,\n",
       " 'Kyle': 16,\n",
       " 'Liane': 10,\n",
       " 'Mr. Garrison': 13,\n",
       " 'Mr. Mackey': 8,\n",
       " 'Mrs. Garrison': 4,\n",
       " 'Randy': 2,\n",
       " 'Sharon': 9,\n",
       " 'Sheila': 14,\n",
       " 'Stan': 15,\n",
       " 'Stephen': 6,\n",
       " 'Token': 17,\n",
       " 'Wendy': 1}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_array = set(train_df['character'])\n",
    "labels_dict = {l: i for i, l in enumerate(labels_array)}\n",
    "inv_labels_dict = {v: k for k, v in labels_dict.items()}\n",
    "\n",
    "NUM_OF_LABELS = len(labels_dict)\n",
    "labels_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, string\n",
    "\n",
    "translator = str.maketrans(\"\",\"\", string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def my_tokenizer(s):\n",
    "#     return [ x for x in nltk.tokenize.word_tokenize(s.lower().translate(translator)) if not x in stopwords ]\n",
    "    return [ x for x in nltk.tokenize.word_tokenize(s.lower().translate(translator))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, name=None):\n",
    "    if not name is None:\n",
    "        print(name)\n",
    "    model.fit(X_train, y_train)\n",
    "    print(\"Train set: %.5f\\tTest set: %.5f\" % (model.score(X_train, y_train), model.score(X_test, y_test)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python36\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "embbedings = KeyedVectors.load_word2vec_format('data/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vector(tokens):\n",
    "    X = [embbedings[tok] for tok in tokens if tok in embbedings]\n",
    "    ans = np.sum(X , axis=0)\n",
    "    if ans.shape != (300, ):\n",
    "        return [0] * 300\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Google word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array([get_vector(list(my_tokenizer(text))) for text in train_df['text']])\n",
    "y_train = np.zeros((len(train_df.index), NUM_OF_LABELS ))\n",
    "\n",
    "for i , name in enumerate(train_df['character']):\n",
    "    y_train[i][labels_dict[name]] = 1\n",
    "\n",
    "X_test = np.array([get_vector(my_tokenizer(text)) for text in test_df['text']])\n",
    "y_test = np.zeros((len(test_df.index), NUM_OF_LABELS ))\n",
    "                  \n",
    "for i , name in enumerate(test_df['character']):\n",
    "    y_test[i][labels_dict[name]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying some Dense Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_shape=(300,)))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(NUM_OF_LABELS, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19329 samples, validate on 1301 samples\n",
      "Epoch 1/20\n",
      "19329/19329 [==============================] - 3s 154us/step - loss: 2.4133 - acc: 0.2144 - val_loss: 2.2149 - val_acc: 0.2321\n",
      "Epoch 2/20\n",
      "19329/19329 [==============================] - 3s 143us/step - loss: 2.2152 - acc: 0.2237 - val_loss: 2.1788 - val_acc: 0.2452\n",
      "Epoch 3/20\n",
      "19329/19329 [==============================] - 3s 141us/step - loss: 2.1583 - acc: 0.2393 - val_loss: 2.1877 - val_acc: 0.2398\n",
      "Epoch 4/20\n",
      "19329/19329 [==============================] - 3s 139us/step - loss: 2.1083 - acc: 0.2566 - val_loss: 2.1704 - val_acc: 0.2613\n",
      "Epoch 5/20\n",
      "19329/19329 [==============================] - 3s 141us/step - loss: 2.0576 - acc: 0.2865 - val_loss: 2.2017 - val_acc: 0.2813\n",
      "Epoch 6/20\n",
      "19329/19329 [==============================] - 3s 141us/step - loss: 2.0199 - acc: 0.3063 - val_loss: 2.1948 - val_acc: 0.2882\n",
      "Epoch 7/20\n",
      "19329/19329 [==============================] - 3s 140us/step - loss: 1.9829 - acc: 0.3175 - val_loss: 2.1919 - val_acc: 0.2806\n",
      "Epoch 8/20\n",
      "19329/19329 [==============================] - 3s 140us/step - loss: 1.9549 - acc: 0.3291 - val_loss: 2.1589 - val_acc: 0.2967\n",
      "Epoch 9/20\n",
      "19329/19329 [==============================] - 3s 141us/step - loss: 1.9236 - acc: 0.3389 - val_loss: 2.1848 - val_acc: 0.2821\n",
      "Epoch 10/20\n",
      "19329/19329 [==============================] - 3s 145us/step - loss: 1.8977 - acc: 0.3519 - val_loss: 2.2749 - val_acc: 0.2798\n",
      "Epoch 11/20\n",
      "19329/19329 [==============================] - 3s 147us/step - loss: 1.8708 - acc: 0.3598 - val_loss: 2.2082 - val_acc: 0.2767\n",
      "Epoch 12/20\n",
      "19329/19329 [==============================] - 3s 165us/step - loss: 1.8499 - acc: 0.3657 - val_loss: 2.3507 - val_acc: 0.2775\n",
      "Epoch 13/20\n",
      "19329/19329 [==============================] - 3s 142us/step - loss: 1.8241 - acc: 0.3774 - val_loss: 2.3101 - val_acc: 0.2667\n",
      "Epoch 14/20\n",
      "19329/19329 [==============================] - 3s 144us/step - loss: 1.8016 - acc: 0.3844 - val_loss: 2.2358 - val_acc: 0.2998\n",
      "Epoch 15/20\n",
      "19329/19329 [==============================] - 3s 161us/step - loss: 1.7839 - acc: 0.3934 - val_loss: 2.3051 - val_acc: 0.2813\n",
      "Epoch 16/20\n",
      "19329/19329 [==============================] - 3s 164us/step - loss: 1.7628 - acc: 0.4023 - val_loss: 2.2610 - val_acc: 0.2936\n",
      "Epoch 17/20\n",
      "19329/19329 [==============================] - 3s 175us/step - loss: 1.7408 - acc: 0.4108 - val_loss: 2.3189 - val_acc: 0.3005\n",
      "Epoch 18/20\n",
      "19329/19329 [==============================] - 3s 142us/step - loss: 1.7222 - acc: 0.4174 - val_loss: 2.2513 - val_acc: 0.2905\n",
      "Epoch 19/20\n",
      "19329/19329 [==============================] - 3s 152us/step - loss: 1.7011 - acc: 0.4269 - val_loss: 2.2690 - val_acc: 0.2898\n",
      "Epoch 20/20\n",
      "19329/19329 [==============================] - 3s 151us/step - loss: 1.6822 - acc: 0.4324 - val_loss: 2.3718 - val_acc: 0.2859\n",
      "Test loss: 2.37178349568\n",
      "Test accuracy: 0.285933897014\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "#           batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))\n",
    "\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desicion Tree Clasifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([labels_dict[name] for name in train_df['character']])\n",
    "y_test = np.array([labels_dict[name] for name in test_df['character']])\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19139123750960799"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(max_depth=17, random_state=0)\n",
    "clf = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25365103766333591"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CountVectorizer / TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(data_frame):\n",
    "    return [ tok for line in data_frame['text'] for tok in my_tokenizer(line)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set(get_tokens(train_df)).union(set(get_tokens(test_df)))\n",
    "vocab = {tok : i for i , tok in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(input='content', encoding='latin1', min_df=2, vocabulary=vocab, tokenizer =my_tokenizer )\n",
    "\n",
    "X_train = cv.fit_transform(train_df['text'])\n",
    "X_test = cv.fit_transform(test_df['text'])\n",
    "y_train = np.array([labels_dict[name] for name in train_df['character']])\n",
    "y_test = np.array([labels_dict[name] for name in test_df['character']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Train set: 0.48021\tTest set: 0.30515\n",
      "BernoulliNB\n",
      "Train set: 0.42134\tTest set: 0.30208\n",
      "LogisticRegression\n",
      "Train set: 0.64514\tTest set: 0.36357\n"
     ]
    }
   ],
   "source": [
    "test_model(MultinomialNB(), 'MultinomialNB')\n",
    "test_model(BernoulliNB(), 'BernoulliNB')\n",
    "test_model(LogisticRegression(), 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best Result (Logistic Regresion over TfidfVectorizer -->  0.37325)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(input='content', encoding='latin1', min_df=2, max_df=200, ngram_range=(1, 2), tokenizer=my_tokenizer, vocabulary=vocab,)\n",
    "X_train = tfv.fit_transform(train_df['text'])\n",
    "X_test = tfv.fit_transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      "Train set: 0.41047\tTest set: 0.29362\n",
      "BernoulliNB\n",
      "Train set: 0.42134\tTest set: 0.30208\n",
      "LogisticRegression\n",
      "Train set: 0.49909\tTest set: 0.37325\n"
     ]
    }
   ],
   "source": [
    "test_model(MultinomialNB(), 'MultinomialNB')\n",
    "test_model(BernoulliNB(), 'BernoulliNB')\n",
    "test_model(LogisticRegression(), 'LogisticRegression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM and Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = 400\n",
    "\n",
    "def get_vector(sentence):\n",
    "    ans = np.zeros(top_words)\n",
    "    ids = [vocab[tok] for tok in my_tokenizer(sentence)]\n",
    "    for i in range(min(top_words, len(ids))):\n",
    "        ans[i] = ids[i]\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Cartman': 4811, 'Stan': 3898, 'Kyle': 3524, 'Butters': 1336, 'Randy': 1233, 'Mr. Garrison': 512, 'Sharon': 442, 'Chef': 433, 'Kenny': 375, 'Liane': 340, 'Jimmy': 332, 'Sheila': 298, 'Gerald': 287, 'Jimbo': 283, 'Wendy': 276, 'Mr. Mackey': 245, 'Announcer': 183, 'Stephen': 177, 'Mrs. Garrison': 174, 'Token': 170})\n",
      "Counter({'Cartman': 308, 'Stan': 272, 'Kyle': 232, 'Butters': 140, 'Randy': 140, 'Stephen': 38, 'Sharon': 35, 'Kenny': 30, 'Mr. Mackey': 21, 'Gerald': 19, 'Token': 17, 'Jimmy': 13, 'Liane': 11, 'Sheila': 9, 'Jimbo': 6, 'Announcer': 5, 'Mr. Garrison': 4, 'Wendy': 1})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "X_train = np.array([get_vector(line) for line in train_df['text']])\n",
    "X_test = np.array([get_vector(line) for line in test_df['text']])\n",
    "\n",
    "y_train = np.zeros((len(train_df.index), NUM_OF_LABELS ))\n",
    "for i , name in enumerate(train_df['character']):\n",
    "    y_train[i][labels_dict[name]] = 1\n",
    "\n",
    "y_test = np.zeros((len(test_df.index), NUM_OF_LABELS ))\n",
    "for i , name in enumerate(test_df['character']):\n",
    "    y_test[i][labels_dict[name]] = 1\n",
    "    \n",
    "print(Counter(train_df['character']))\n",
    "print(Counter(test_df['character']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, None, 64)          25600     \n",
      "_________________________________________________________________\n",
      "lstm_45 (LSTM)               (None, None, 32)          12416     \n",
      "_________________________________________________________________\n",
      "lstm_46 (LSTM)               (None, 16)                3136      \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 128)               2176      \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 20)                2580      \n",
      "=================================================================\n",
      "Total params: 45,908\n",
      "Trainable params: 45,908\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 19329 samples, validate on 1301 samples\n",
      "Epoch 1/1\n",
      "19329/19329 [==============================] - 453s 23ms/step - loss: 2.3461 - acc: 0.2413 - val_loss: 2.2171 - val_acc: 0.2367\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x20e7057e908>"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_vecor_length = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(top_words, embedding_vecor_length))\n",
    "model.add(LSTM(32, dropout=0.3, recurrent_dropout=0.3, return_sequences=True))\n",
    "model.add(LSTM(16, dropout=0.3, recurrent_dropout=0.3))\n",
    "model.add(Dense(128))\n",
    "model.add(Dense(NUM_OF_LABELS, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=1, batch_size=64)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
